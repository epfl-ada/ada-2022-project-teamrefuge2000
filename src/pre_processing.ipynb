{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER           = \"../data/\"\n",
    "PROCESSED_DATA_FOLDER = '../processed_data/'\n",
    "CORENLP_DATA_FOLDER   = '../corenlp_plot_summaries/'\n",
    "\n",
    "CHARACTER_DATASET     = DATA_FOLDER + \"character.metadata.tsv\"\n",
    "MOVIE_DATASET         = DATA_FOLDER + \"movie.metadata.tsv\"\n",
    "NAME_CLUSTER_DATASET  = DATA_FOLDER + \"name.clusters.txt\"\n",
    "PLOT_DATASET          = DATA_FOLDER + \"plot_summaries.txt\"\n",
    "TVTROPES_DATASET      = DATA_FOLDER + \"tvtropes.clusters.txt\"\n",
    "\n",
    "PROCESSED_CHARACTER   = PROCESSED_DATA_FOLDER + 'character_metadata.csv'\n",
    "PROCESSED_MOVIE       = PROCESSED_DATA_FOLDER + 'movie_metadata.csv'\n",
    "PROCESSED_NAME        = PROCESSED_DATA_FOLDER + 'name_clusters.csv'\n",
    "PROCESSED_PLOT        = PROCESSED_DATA_FOLDER + 'plot_summaries.csv'\n",
    "PROCESSED_TVTROPES    = PROCESSED_DATA_FOLDER + 'tvtropes_clusters.csv'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing the data\n",
    "\n",
    "At first, we fetch the data from the `data` folder into pd dataframes in order to set the desired headings and to modify the type if needed.\n",
    "Then we store it in `processed_data` folder in the desired .csv format."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For `movie_metadata` and `character_metadata` we modify the dates to the datetime format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_metadata = pd.read_csv(MOVIE_DATASET, sep='\\t', header=None) \\\n",
    "    .rename(columns={0 : 'wikipedia_movie_id',\n",
    "                     1 : 'freebase_movie_id',\n",
    "                     2 : 'movie_name',\n",
    "                     3 : 'movie_release_date',\n",
    "                     4 : 'office_revenue',\n",
    "                     5 : 'runtime',\n",
    "                     6 : 'languages',\n",
    "                     7 : 'countries',\n",
    "                     8 : 'genres'})\n",
    "\n",
    "movie_metadata['movie_release_date'] = pd.to_datetime(movie_metadata['movie_release_date'], errors = 'coerce')\n",
    "movie_metadata['movie_release_year'] = movie_metadata['movie_release_date'].apply(lambda d : d.year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_metadata = pd.read_csv(CHARACTER_DATASET, sep='\\t', header=None) \\\n",
    "    .rename(columns={0  : 'wikipedia_movie_id',\n",
    "                     1  : 'freebase_movie_id',\n",
    "                     2  : 'movie_release_date',\n",
    "                     3  : 'character_name',\n",
    "                     4  : 'actor_birth_date',\n",
    "                     5  : 'actor_gender',\n",
    "                     6  : 'actor_height',\n",
    "                     7  : 'actor_ethnicity',\n",
    "                     8  : 'actor_name',\n",
    "                     9  : 'actor_age',\n",
    "                     10 : 'freebase_map_id',\n",
    "                     11 : 'freebase_character_id',\n",
    "                     12 : 'freebase_actor_id'})\n",
    "\n",
    "character_metadata['movie_release_date'] = pd.to_datetime(character_metadata['movie_release_date'],          errors='coerce')\n",
    "character_metadata['actor_birth_date'] =   pd.to_datetime(character_metadata['actor_birth_date'], utc=True , errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_summaries = pd.read_csv(PLOT_DATASET, sep='\\t', header=None)\\\n",
    "    .rename(columns={0 : 'movie_id',\n",
    "                     1 : 'plot'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_clusters = pd.read_csv(NAME_CLUSTER_DATASET, sep='\\t', header=None)\\\n",
    "    .rename(columns={0 : 'character_name',\n",
    "                     1 : 'freebase_map_id'})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the `tvtropes_cluster` : we first create a raw dataframe containing the data.\n",
    "Then we construct the resulting dataframe where each values of the dictionary correspond to the correct column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvtropes_cluster_raw = pd.read_csv(TVTROPES_DATASET, sep='\\t', header=None)\n",
    "tvtropes_cluster_raw[1] = tvtropes_cluster_raw[1].map(ast.literal_eval)\n",
    "tvtropes_cluster = pd.DataFrame()\n",
    "\n",
    "tvtropes_cluster['character_type'] = tvtropes_cluster_raw[0]\n",
    "tvtropes_cluster['character_name'] = tvtropes_cluster_raw[1].map(lambda d : d['char'])\n",
    "tvtropes_cluster['movie_name']     = tvtropes_cluster_raw[1].map(lambda d : d['movie'])\n",
    "tvtropes_cluster['movie_id']       = tvtropes_cluster_raw[1].map(lambda d : d['id'])\n",
    "tvtropes_cluster['actor_name']     = tvtropes_cluster_raw[1].map(lambda d : d['actor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_metadata.to_csv(PROCESSED_CHARACTER, index=False)\n",
    "movie_metadata    .to_csv(PROCESSED_MOVIE,     index=False)\n",
    "plot_summaries    .to_csv(PROCESSED_PLOT,      index=False)\n",
    "name_clusters     .to_csv(PROCESSED_NAME,      index=False)\n",
    "tvtropes_cluster  .to_csv(PROCESSED_TVTROPES,  index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using dictionary of English words tagged with their natural gender\n",
    "# source : https://github.com/ecmonsen/gendered_words\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/ecmonsen/gendered_words/master/gendered_words.json'\n",
    "tagged_words = pd.read_json(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORDS_M = set(tagged_words[tagged_words.gender == 'm']['word'])\n",
    "WORDS_F = set(tagged_words[tagged_words.gender == 'f']['word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_names_m_f(movie_id):\n",
    "    names     = character_metadata[character_metadata.wikipedia_movie_id == movie_id][['character_name', 'actor_gender']]\\\n",
    "                .dropna()\\\n",
    "                .groupby('actor_gender')\\\n",
    "                .agg(' '.join)\\\n",
    "                .to_dict()['character_name']\n",
    "\n",
    "    names_m   = set(names.get('M', '').lower().split(' '))\n",
    "    names_f   = set(names.get('F', '').lower().split(' '))\n",
    "    intersect = names_f.intersection(names_m)\n",
    "\n",
    "    names_m   = names_m.difference(intersect)\n",
    "    names_f   = names_f.difference(intersect)\n",
    "    return names_m, names_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_movie(movie_id, root):\n",
    "    names_m, names_f = get_names_m_f(movie_id)\n",
    "\n",
    "    actions_by_m = []\n",
    "    actions_by_f = []\n",
    "    actions_on_m = []\n",
    "    actions_on_f = []\n",
    "    descriptions_of_m = []\n",
    "    descriptions_of_f = []\n",
    "\n",
    "    if root != 'FileNotFoundError':\n",
    "        for dep in root.iter('dep'):\n",
    "\n",
    "            subject_name = dep[1].text.lower()\n",
    "            action       = dep[0].text\n",
    "            relation     = dep.attrib['type']\n",
    "\n",
    "            if   subject_name in names_m or subject_name in WORDS_M :\n",
    "                if   relation in {'nsubj'}:\n",
    "                    actions_by_m     .append(action)\n",
    "                elif relation in {'dobj', 'iobj'}:\n",
    "                    actions_on_m     .append(action)\n",
    "                elif relation in {'nmod', 'amod', 'nummod', 'appos'}:\n",
    "                    descriptions_of_m.append(action)\n",
    "\n",
    "            elif subject_name in names_f or subject_name in WORDS_F :\n",
    "                if   relation in {'nsubj'}:\n",
    "                    actions_by_f     .append(action)\n",
    "                elif relation in {'dobj', 'iobj'}:\n",
    "                    actions_on_f     .append(action)\n",
    "                elif relation in {'nmod', 'amod', 'nummod', 'appos'}:\n",
    "                    descriptions_of_f.append(action)\n",
    "\n",
    "    new_row = {\n",
    "        'actions by men'        : actions_by_m,\n",
    "        'actions by women'      : actions_by_f,\n",
    "        'actions on men'        : actions_on_m,\n",
    "        'actions on women'      : actions_on_f,\n",
    "        'descriptions of men'   : descriptions_of_m,\n",
    "        'descriptions of women' : descriptions_of_f\n",
    "    }\n",
    "    return new_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_ids = set(movie_metadata['wikipedia_movie_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_nlp(id) -> ET :\n",
    "    try:\n",
    "        tree = ET.parse(CORENLP_DATA_FOLDER + str(id) + '.xml')\n",
    "        return tree.getroot()\n",
    "    #It is quite common that some ids do not have a corresponding xml file\n",
    "    except FileNotFoundError:\n",
    "        return 'FileNotFoundError'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes approximately 20min to run\n",
    "nlp_dict = {}\n",
    "for movie_id in movie_ids:\n",
    "    root = parse_nlp(movie_id)\n",
    "    if root != 'FileNotFoundError':\n",
    "        nlp_dict[movie_id] = analyze_movie(movie_id, root)\n",
    "movie_nlp = pd.DataFrame.from_dict(nlp_dict, orient='index')\n",
    "movie_nlp.index.name = 'movie_id'\n",
    "movie_nlp.to_csv(PROCESSED_DATA_FOLDER + 'movie_nlp.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
